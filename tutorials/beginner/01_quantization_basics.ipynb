{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Quantization Basics: Your First Steps into LLM Optimization\n",
    "\n",
    "Welcome to the world of Large Language Model optimization! This interactive tutorial will teach you the fundamentals of quantization - the art and science of making AI models smaller, faster, and more efficient.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "- What quantization is and why it matters\n",
    "- Different types of quantization (4-bit, 8-bit, 16-bit)\n",
    "- Hands-on quantization of your first model\n",
    "- How to measure and compare results\n",
    "- Real-world applications and use cases\n",
    "\n",
    "## ‚è±Ô∏è Time Required: 30-45 minutes\n",
    "## üìã Prerequisites: Basic Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç What is Quantization?\n",
    "\n",
    "Imagine you have a high-resolution photo that takes up 10MB of space. You can compress it to 2MB with minimal visual quality loss. Quantization does something similar for AI models - it reduces the precision of numbers used in the model to save memory and increase speed.\n",
    "\n",
    "### The Magic Numbers\n",
    "- **32-bit (FP32)**: Full precision - like a 4K photo\n",
    "- **16-bit (FP16)**: Half precision - like 1080p video\n",
    "- **8-bit (INT8)**: Quarter precision - like a compressed image\n",
    "- **4-bit (INT4)**: Ultra compression - like a thumbnail\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the basics - understanding number precision\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample \"weight\" - this represents a tiny piece of an AI model\n",
    "original_weight = torch.tensor([3.14159265, 2.71828182, 1.41421356, 0.57721566])\n",
    "print(f\"Original weights (32-bit): {original_weight}\")\n",
    "\n",
    "# Convert to different precisions\n",
    "weight_16bit = original_weight.half()  # 16-bit\n",
    "weight_8bit = torch.round(original_weight * 127) / 127  # Simulated 8-bit\n",
    "weight_4bit = torch.round(original_weight * 7) / 7  # Simulated 4-bit\n",
    "\n",
    "print(f\"16-bit weights: {weight_16bit}\")\n",
    "print(f\"8-bit weights:  {weight_8bit}\")\n",
    "print(f\"4-bit weights:  {weight_4bit}\")\n",
    "\n",
    "# Calculate memory savings\n",
    "memory_32bit = original_weight.element_size() * original_weight.numel()\n",
    "memory_16bit = weight_16bit.element_size() * weight_16bit.numel()\n",
    "memory_8bit = memory_32bit // 4  # 8-bit uses 1/4 the memory\n",
    "memory_4bit = memory_32bit // 8  # 4-bit uses 1/8 the memory\n",
    "\n",
    "print(f\"\\nüíæ Memory Usage:\")\n",
    "print(f\"32-bit: {memory_32bit} bytes\")\n",
    "print(f\"16-bit: {memory_16bit} bytes ({memory_32bit/memory_16bit:.1f}x smaller)\")\n",
    "print(f\"8-bit:  {memory_8bit} bytes ({memory_32bit/memory_8bit:.1f}x smaller)\")\n",
    "print(f\"4-bit:  {memory_4bit} bytes ({memory_32bit/memory_4bit:.1f}x smaller)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Visualizing Quantization\n",
    "\n",
    "Let's create a visual representation of how quantization affects data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of quantization effects\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate smooth curve (representing model weights)\n",
    "x = np.linspace(0, 4*np.pi, 1000)\n",
    "y_original = np.sin(x) + 0.5 * np.sin(3*x)\n",
    "\n",
    "# Simulate different quantization levels\n",
    "def quantize(data, levels):\n",
    "    min_val, max_val = data.min(), data.max()\n",
    "    scale = (max_val - min_val) / (levels - 1)\n",
    "    quantized = np.round((data - min_val) / scale) * scale + min_val\n",
    "    return quantized\n",
    "\n",
    "y_32bit = y_original  # No quantization\n",
    "y_8bit = quantize(y_original, 256)  # 8-bit = 256 levels\n",
    "y_4bit = quantize(y_original, 16)   # 4-bit = 16 levels\n",
    "y_2bit = quantize(y_original, 4)    # 2-bit = 4 levels\n",
    "\n",
    "# Plot comparisons\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y_32bit, 'b-', linewidth=2, label='32-bit (Original)')\n",
    "plt.title('32-bit: Full Precision')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y_original, 'b-', alpha=0.5, label='Original')\n",
    "plt.plot(x, y_8bit, 'r-', linewidth=2, label='8-bit')\n",
    "plt.title('8-bit: Minimal Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y_original, 'b-', alpha=0.5, label='Original')\n",
    "plt.plot(x, y_4bit, 'g-', linewidth=2, label='4-bit')\n",
    "plt.title('4-bit: Noticeable but Acceptable')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, y_original, 'b-', alpha=0.5, label='Original')\n",
    "plt.plot(x, y_2bit, 'm-', linewidth=2, label='2-bit')\n",
    "plt.title('2-bit: Significant Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('üìä How Quantization Affects Data Quality', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display error metrics\n",
    "mse_8bit = np.mean((y_original - y_8bit)**2)\n",
    "mse_4bit = np.mean((y_original - y_4bit)**2)\n",
    "mse_2bit = np.mean((y_original - y_2bit)**2)\n",
    "\n",
    "print(f\"üìà Quantization Error (Mean Squared Error):\")\n",
    "print(f\"8-bit:  {mse_8bit:.6f} (Excellent)\")\n",
    "print(f\"4-bit:  {mse_4bit:.6f} (Good)\")\n",
    "print(f\"2-bit:  {mse_2bit:.6f} (Poor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Your First Model Quantization\n",
    "\n",
    "Now let's quantize a real language model! We'll start with a small model so it runs quickly on any hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this if you haven't already)\n",
    "# !pip install transformers torch bitsandbytes accelerate\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Choose a small model for this tutorial\n",
    "model_name = \"microsoft/DialoGPT-small\"  # Only 117M parameters\n",
    "\n",
    "print(f\"ü§ñ Loading model: {model_name}\")\n",
    "print(\"This might take a minute the first time...\")\n",
    "\n",
    "# Load the original model (32-bit)\n",
    "start_time = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_original = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "load_time_original = time.time() - start_time\n",
    "\n",
    "# Calculate original model size\n",
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return total_params, total_size\n",
    "\n",
    "params_orig, size_orig = get_model_size(model_original)\n",
    "\n",
    "print(f\"\\nüìä Original Model Stats:\")\n",
    "print(f\"Parameters: {params_orig:,}\")\n",
    "print(f\"Size: {size_orig / 1024**2:.1f} MB\")\n",
    "print(f\"Load time: {load_time_original:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a quantized version!\n",
    "print(\"üîß Creating 8-bit quantized model...\")\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0  # Threshold for outlier detection\n",
    ")\n",
    "\n",
    "# Load quantized model\n",
    "start_time = time.time()\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "load_time_8bit = time.time() - start_time\n",
    "\n",
    "# Calculate quantized model size (approximate)\n",
    "params_8bit, size_8bit = get_model_size(model_8bit)\n",
    "# Note: The actual memory usage is lower due to 8-bit storage\n",
    "estimated_8bit_size = size_orig / 4  # 8-bit uses ~1/4 the memory\n",
    "\n",
    "print(f\"\\nüìä 8-bit Model Stats:\")\n",
    "print(f\"Parameters: {params_8bit:,} (same as original)\")\n",
    "print(f\"Estimated size: {estimated_8bit_size / 1024**2:.1f} MB\")\n",
    "print(f\"Load time: {load_time_8bit:.2f} seconds\")\n",
    "print(f\"Memory savings: {size_orig / estimated_8bit_size:.1f}x smaller! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing Model Quality\n",
    "\n",
    "The big question: Does our quantized model still work well? Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Tell me a joke\",\n",
    "    \"Explain artificial intelligence\"\n",
    "]\n",
    "\n",
    "def generate_response(model, prompt, max_length=50):\n",
    "    \"\"\"Generate a response from the model\"\"\"\n",
    "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from the response\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response, generation_time\n",
    "\n",
    "print(\"üß™ Testing Model Quality\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüî∏ Test {i}: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Test original model\n",
    "    response_orig, time_orig = generate_response(model_original, prompt)\n",
    "    print(f\"Original (32-bit): {response_orig}\")\n",
    "    print(f\"Time: {time_orig:.3f}s\")\n",
    "    \n",
    "    # Test quantized model\n",
    "    response_8bit, time_8bit = generate_response(model_8bit, prompt)\n",
    "    print(f\"Quantized (8-bit): {response_8bit}\")\n",
    "    print(f\"Time: {time_8bit:.3f}s\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    speedup = time_orig / time_8bit if time_8bit > 0 else 1\n",
    "    print(f\"Speedup: {speedup:.2f}x {'üöÄ' if speedup > 1 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Comprehensive Comparison\n",
    "\n",
    "Let's create a detailed comparison of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison\n",
    "import pandas as pd\n",
    "\n",
    "# Collect metrics\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Parameters',\n",
    "        'Model Size (MB)',\n",
    "        'Load Time (s)',\n",
    "        'Memory Usage',\n",
    "        'Precision',\n",
    "        'Compression Ratio'\n",
    "    ],\n",
    "    'Original (32-bit)': [\n",
    "        f\"{params_orig:,}\",\n",
    "        f\"{size_orig / 1024**2:.1f}\",\n",
    "        f\"{load_time_original:.2f}\",\n",
    "        \"High\",\n",
    "        \"Full (FP32)\",\n",
    "        \"1x (baseline)\"\n",
    "    ],\n",
    "    'Quantized (8-bit)': [\n",
    "        f\"{params_8bit:,}\",\n",
    "        f\"{estimated_8bit_size / 1024**2:.1f}\",\n",
    "        f\"{load_time_8bit:.2f}\",\n",
    "        \"Low\",\n",
    "        \"Reduced (INT8)\",\n",
    "        f\"{size_orig / estimated_8bit_size:.1f}x smaller\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"üìä Model Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Create a visual comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Memory usage comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['Original\\n(32-bit)', 'Quantized\\n(8-bit)']\n",
    "sizes = [size_orig / 1024**2, estimated_8bit_size / 1024**2]\n",
    "colors = ['#ff7f7f', '#7fbf7f']\n",
    "\n",
    "bars = plt.bar(models, sizes, color=colors, alpha=0.8)\n",
    "plt.title('üíæ Memory Usage Comparison')\n",
    "plt.ylabel('Size (MB)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{size:.1f} MB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Speed comparison (if we have timing data)\n",
    "plt.subplot(1, 2, 2)\n",
    "load_times = [load_time_original, load_time_8bit]\n",
    "bars = plt.bar(models, load_times, color=colors, alpha=0.8)\n",
    "plt.title('‚ö° Load Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, load_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"‚úÖ Memory reduced by {size_orig / estimated_8bit_size:.1f}x\")\n",
    "print(f\"‚úÖ Model still generates coherent responses\")\n",
    "print(f\"‚úÖ Load time: {load_time_8bit:.2f}s vs {load_time_original:.2f}s\")\n",
    "print(f\"‚úÖ Same number of parameters, just stored more efficiently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You've Learned\n",
    "\n",
    "Congratulations! You've just completed your first quantization experiment. Here's what you accomplished:\n",
    "\n",
    "### ‚úÖ Key Concepts Mastered\n",
    "1. **Quantization Basics**: Converting high-precision numbers to lower precision\n",
    "2. **Memory Efficiency**: How quantization reduces model size\n",
    "3. **Quality Trade-offs**: Understanding the balance between size and performance\n",
    "4. **Practical Implementation**: Using real tools to quantize models\n",
    "\n",
    "### üîç What We Observed\n",
    "- **Memory Savings**: ~4x reduction in memory usage\n",
    "- **Quality Retention**: Model still generates coherent responses\n",
    "- **Speed Benefits**: Faster loading and potentially faster inference\n",
    "- **Minimal Setup**: Easy to implement with existing tools\n",
    "\n",
    "### üöÄ Real-World Applications\n",
    "- **Mobile Deployment**: Run larger models on phones/tablets\n",
    "- **Edge Computing**: Deploy AI in resource-constrained environments\n",
    "- **Cost Reduction**: Use smaller, cheaper GPUs for inference\n",
    "- **Batch Processing**: Process more requests simultaneously\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "Ready to dive deeper? Here are your next learning opportunities:\n",
    "\n",
    "### Immediate Next Steps (Choose One)\n",
    "1. **[Tutorial 2: Advanced Quantization](./02_advanced_quantization.ipynb)** - Learn about 4-bit quantization and QLoRA\n",
    "2. **[Tutorial 3: Model Comparison](./03_model_comparison.ipynb)** - Compare different quantization methods\n",
    "3. **[Interactive Examples](../../examples/interactive/)** - Try quantization on different model types\n",
    "\n",
    "### Longer-Term Learning Path\n",
    "1. **Intermediate**: Explore GPTQ, AWQ, and other advanced methods\n",
    "2. **Advanced**: Learn about abliteration and model modification\n",
    "3. **Expert**: Contribute to research and develop new techniques\n",
    "\n",
    "## ü§ù Get Help & Connect\n",
    "\n",
    "- **Questions?** Open an issue on our [GitHub repository](https://github.com/your-repo/issues)\n",
    "- **Discussions**: Join our [community forum](https://github.com/your-repo/discussions)\n",
    "- **Updates**: Follow our [research blog](https://your-blog.com)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing your first quantization tutorial! You're now equipped with the fundamental knowledge to optimize language models efficiently.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Bonus: Experiment Ideas\n",
    "\n",
    "Want to explore further? Try these experiments:\n",
    "\n",
    "### Experiment 1: Different Model Sizes\n",
    "```python\n",
    "# Try quantizing different sized models\n",
    "models_to_try = [\n",
    "    \"microsoft/DialoGPT-small\",   # 117M parameters\n",
    "    \"microsoft/DialoGPT-medium\",  # 345M parameters\n",
    "    \"gpt2\",                       # 124M parameters\n",
    "    \"gpt2-medium\"                 # 355M parameters\n",
    "]\n",
    "```\n",
    "\n",
    "### Experiment 2: 4-bit Quantization\n",
    "```python\n",
    "# Try even more aggressive quantization\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "```\n",
    "\n",
    "### Experiment 3: Quality Metrics\n",
    "```python\n",
    "# Implement more sophisticated quality measurements\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use perplexity or other metrics to measure quality loss\n",
    "```\n",
    "\n",
    "Happy experimenting! üß™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}